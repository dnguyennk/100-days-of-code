# -*- coding: utf-8 -*-
"""support_vector_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wy9-7PGbIKJnWrPiHQRAAzmMcyL3-bdc

# Support Vector Regression (SVR)

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

print(X)

print(y)

"""Note: X is 2D array, while y is 1D array => we will reshape y into 2D array in order to have 1 unique input."""

y = y.reshape(len(y), 1)   #changed row into column, reshape(new_rows, new_cols)
print(y)

"""## Feature Scaling

Steps:

1.   Apply feature scaling on matrix of feature X (X_train and X_test).
2.   Apply feature scaling on dependent variable y (since the salary is much higher than the position level)

Note that if we do that our SVR model will not work properly.

When we will not apply feature scaling?
- On dummy variables (result from one hot encoding)
- Dependent variable that take binary value (b/c the value are already in the right range)

When we will apply feature scaling?
- When the dependent variable are super large compare to independent variables (such as this case)
- Whenever we slipt the training set and the test set, we will apply feature scaling after the split
"""

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()   #feature scaling
sc_y = StandardScaler()   #dependent variable scaling
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y)

print(X)   #examine X

print(y)   #examine y, range[-3, 3]

"""## Training the SVR model on the whole dataset

More about Kernel Function: [SVM Kernel & Examples](https://data-flair.training/blogs/svm-kernel-functions/)

In this exercise, we will use Gaussian Tadial Basic Function (RBF) - The most widely used.
"""

from sklearn.svm import SVR
regressor = SVR(kernel= 'rbf')
regressor.fit(X, y)   # whole dataset

"""## Predicting a new result

In this section, we will understand how to reverse the scaling of our predictions. Since we already transform the new scale of y in the previous section, so the result we get is not the orginal y.
"""

# regressor.predict(sc_X.transform([[6.5]])) -  expect 2D array input

"""Since we train our model on the scale value of the training set, we must enter the scaled value of the input that we want to predict.


"""

# reverse scaling
sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])).reshape(-1,1))  # add reshape to avoid formating error

"""## Visualising the SVR results"""

plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'pink')
plt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X).reshape(-1,1)) , color = 'green')
plt.title('Truth or Bluff (Linear Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""Note about plt.plot():
- 1st parameter is the reversed transformation
- 2nd parameter is the preditions of the input in X, which we must not reapply the transform method on X (1st para is already scale)

Analyze the result: The predictions in green are very close to the real outcomes in pink. Expect the outlier (the last one).

## Visualising the SVR results (for higher resolution and smoother curve)
"""

X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)   #original scale
X_grid = X_grid.reshape((len(X_grid), 1))     # back to original shape
plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'pink') #inverse for both x, y
plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid)).reshape(-1,1)), color = 'green')
plt.title('Truth or Bluff (SVR Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""Note for plt.plot(): don't forget to apply X_grid scale!

"""