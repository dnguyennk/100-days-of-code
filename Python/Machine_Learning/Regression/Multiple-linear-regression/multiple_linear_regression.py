# -*- coding: utf-8 -*-
"""Copy of multiple_linear_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F-zPffpMKRpx1dPhxJHUexEFe4eeqdzC

# Multiple Linear Regression

A bit about our dataset:


1.   No missing data
2.   Inlcudes numerical variables and categorical variables

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values   # The first 4 columns
y = dataset.iloc[:, -1].values    # The last columns

"""Let examine our data."""

print(X)

"""## Encoding categorical data"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
# Note that our categorical variable is at index 3
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

print(X)

"""Notice that we have the same 3 columns which correspoding to R&D spend, Admin and Marketing Spend. The state column is encoded as the 1st column here, where [0.0 0.0 1.0] is New York, California is [1.0 0.0 0.0], and Florida is [0.0 1.0 0.0]

In this exercise, we don't have to apply feature scaling since in the equation of the multiple regression has coefficient * each independent variables/features. Hence, it doesn't matter which feature is higher (b/c the coefficients will compensate to put every thing on the same scale).

## Splitting the dataset into the Training set and Test set
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""## Training the Multiple Linear Regression model on the Training set

Aside questions
1. Do we have to do anything to avoid the dummy trap?
Answer: No, because the Multiple Linear Regression model which we use below, will automatically avoid this trap. In other words, our 3 newly encoded columns will be outcasted during the training phrase. (Advanced implementation).
2. Do we have deploy the backward elimination technique to select the feature that has the most statistically significant?
Answer: No, same as above. The model will identify the best features that has the highest P-value => figure out how to predict dependent variables with the highest accuracy
"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

"""## Predicting the Test set results

We will display 2 vectors:


1.   Vector of the real profit of the test set(20% of dataset).
2.   Vector of the 10 predicted profit of the same test set. (y_pred)
"""

y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)  # Easier to visualize
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""- np.concatenate() allows to concatenate either vertically or horizontally 2 vectors (or arrays).
- y_pred.reshape(len(y_pred),1): a tuple that contains 2 vectors, where it reshaped from horizontal to vertical.
- len(y_pred): represent the number of columns.

To sumarize: We want to reshape our y_pred as a vector into an array having len y_pred rows and 1 column.

We do the same trick for y_test.

Note: (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)) is the 1st part of the function. We want it is horizontal => next argument is 1.

 **Analyze the result:**


1.   1st column (y_pred)
2.   2nd column (real profit from test set)
If we analyze each row, we can see that we have some ok predictions (not too far from the result), and some very good prediction. Hence the multiple linear regression is well adapted to this dataset.


"""